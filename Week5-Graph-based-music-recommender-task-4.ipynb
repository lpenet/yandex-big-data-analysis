{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.enableHiveSupport().master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- trackId: integer (nullable = true)\n",
      " |-- artistId: integer (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "playHistoryPath = '/data/sample264'\n",
    "playHistoryGraph = sparkSession.read.parquet(playHistoryPath)\n",
    "playHistoryGraph.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Artist: string (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metaDataPath = '/data/meta'\n",
    "metaDataGraph = sparkSession.read.parquet(metaDataPath)\n",
    "metaDataGraph.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "playHistoryGraph.createTempView(\"history1\")\n",
    "playHistoryGraph.createTempView(\"history2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "consecutiveTracksForUser = sparkSession.sql( \\\n",
    "    \"select h1.trackId as track1, h2.trackId as track2, h1.userId as user \" \\\n",
    "    \"from history1 h1, history2 h2 \" \\\n",
    "    \"where h1.userId = h2.userId \" \\\n",
    "    \"and h1.trackId != h2.trackId \" \\\n",
    "    \"and abs(h2.timestamp - h1.timestamp) <= 420 \" \\\n",
    ").groupBy(col(\"track1\"), col(\"track2\")) \\\n",
    ".count().alias(\"count\") \\\n",
    ".orderBy(col(\"track1\"), col(\"track2\")) \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consecutiveTracksForUser.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, sum\n",
    "\n",
    "window = Window.partitionBy(\"track1\").orderBy(col(\"count\").desc())\n",
    "       \n",
    "topsDF = consecutiveTracksForUser.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= 40) \\\n",
    "        .drop(col(\"row_number\")) \\\n",
    "        .orderBy(col(\"track1\"), col(\"track2\")) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sumsDF = topsDF.groupBy(col(\"track1\")) \\\n",
    ".agg(sum(col(\"count\")).alias(\"sum_weights\")) \\\n",
    ".orderBy(\"track1\") \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sumsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_count = topsDF.join(sumsDF, \"track1\", \"inner\") \\\n",
    "    .withColumn(\"weight\", col(\"count\") / col(\"sum_weights\")) \\\n",
    "    .cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = normalized_count.orderBy(col(\"weight\").desc(), col(\"track1\"), col(\"track2\")).limit(40)\n",
    "#results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.select(col(\"track1\"), col(\"track2\"))\n",
    "for t1, t2 in results.collect():\n",
    "    print(\"{}\\t{}\".format(t1,t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "tracksPerUser = sparkSession.sql( \\\n",
    "    \"select userId as user, trackId as track \" \\\n",
    "    \"from history1 \"\n",
    ").groupBy(col(\"user\"), col(\"track\")) \\\n",
    ".count().alias(\"count\") \\\n",
    ".orderBy(col(\"count\").desc(), col(\"user\"), col(\"track\")) \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracksPerUser.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, sum\n",
    "\n",
    "window = Window.partitionBy(\"user\").orderBy(col(\"count\").desc())\n",
    "       \n",
    "topsTracksPerUser = tracksPerUser.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= 1000) \\\n",
    "        .drop(col(\"row_number\")) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumsTopsTracksPerUser = topsTracksPerUser.groupBy(col(\"user\")) \\\n",
    ".agg(sum(col(\"count\")).alias(\"sum_weights\")) \\\n",
    ".orderBy(\"user\") \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_topsTracksPerUser = topsTracksPerUser.join(sumsTopsTracksPerUser, \"user\", \"inner\") \\\n",
    "    .withColumn(\"norm_weight\", col(\"count\") / col(\"sum_weights\")) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = normalized_topsTracksPerUser.orderBy(col(\"norm_weight\").desc(), col(\"user\"), col(\"track\")) \\\n",
    "    .limit(40) \\\n",
    "    .select(col(\"user\"),col(\"track\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, t in results.collect():\n",
    "    print(\"{} {}\".format(u, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "artistsPerUser = sparkSession.sql( \\\n",
    "    \"select userId as user, artistId as artist \" \\\n",
    "    \"from history1 \"\n",
    ").groupBy(col(\"user\"), col(\"artist\")) \\\n",
    ".count().alias(\"count\") \\\n",
    ".orderBy(col(\"count\").desc(), col(\"user\"), col(\"artist\")) \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+\n",
      "|  user| artist|count|\n",
      "+------+-------+-----+\n",
      "|668849| 994686|  277|\n",
      "|436158|1003021|  142|\n",
      "|442306|1001300|  107|\n",
      "|560428| 975695|   94|\n",
      "|767478| 991179|   94|\n",
      "|278647| 981306|   87|\n",
      "|770607| 978956|   76|\n",
      "| 20167| 978800|   75|\n",
      "| 26976|1003021|   75|\n",
      "|637446| 978288|   71|\n",
      "|397054| 974777|   70|\n",
      "|407962| 968823|   70|\n",
      "|201788| 970240|   69|\n",
      "|343313| 968823|   69|\n",
      "|510688| 976986|   69|\n",
      "|408783| 978956|   67|\n",
      "|714890| 985755|   67|\n",
      "|525436| 976384|   66|\n",
      "|607295| 993789|   63|\n",
      "|300275| 989189|   62|\n",
      "+------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artistsPerUser.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, sum\n",
    "\n",
    "window = Window.partitionBy(\"user\").orderBy(col(\"count\").desc())\n",
    "       \n",
    "topsArtistsPerUser = artistsPerUser.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= 100) \\\n",
    "        .drop(col(\"row_number\")) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumsTopsArtistsPerUser = topsArtistsPerUser.groupBy(col(\"user\")) \\\n",
    ".agg(sum(col(\"count\")).alias(\"sum_weights\")) \\\n",
    ".orderBy(\"user\") \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_topsArtistsPerUser = topsArtistsPerUser.join(sumsTopsArtistsPerUser, \"user\", \"inner\") \\\n",
    "    .withColumn(\"norm_weight\", col(\"count\") / col(\"sum_weights\")) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = normalized_topsArtistsPerUser.orderBy(col(\"norm_weight\").desc(), col(\"user\"), col(\"artist\")) \\\n",
    "    .limit(40) \\\n",
    "    .select(col(\"user\"),col(\"artist\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 993426\n",
      "116 974937\n",
      "128 1003021\n",
      "131 983068\n",
      "195 997265\n",
      "215 991696\n",
      "235 990642\n",
      "288 1000564\n",
      "300 1003362\n",
      "321 986172\n",
      "328 967986\n",
      "333 1000416\n",
      "346 982037\n",
      "356 974846\n",
      "374 1003167\n",
      "428 993161\n",
      "431 969340\n",
      "445 970387\n",
      "488 970525\n",
      "542 969751\n",
      "612 987351\n",
      "617 970240\n",
      "649 973851\n",
      "658 973232\n",
      "662 975279\n",
      "698 995788\n",
      "708 968848\n",
      "746 972032\n",
      "747 972032\n",
      "776 997265\n",
      "784 969853\n",
      "806 995126\n",
      "811 996436\n",
      "837 989262\n",
      "901 988199\n",
      "923 977066\n",
      "934 990860\n",
      "957 991171\n",
      "989 975339\n",
      "999 968823\n"
     ]
    }
   ],
   "source": [
    "for u, t in results.collect():\n",
    "    print(\"{} {}\".format(u, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "tracksPerArtist = sparkSession.sql( \\\n",
    "    \"select artistId as artist, trackId as track \" \\\n",
    "    \"from history1 \"\n",
    ").groupBy(col(\"artist\"), col(\"track\")) \\\n",
    ".count().alias(\"count\") \\\n",
    ".orderBy(col(\"count\").desc(), col(\"artist\"), col(\"track\")) \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "|artist| track|count|\n",
      "+------+------+-----+\n",
      "|987351|886091| 2958|\n",
      "|988199|871513| 2904|\n",
      "|997265|946408| 2836|\n",
      "|981306|864690| 2582|\n",
      "|974503|858904| 2453|\n",
      "|981306|831005| 2308|\n",
      "|970525|841340| 2275|\n",
      "|983300|846624| 2101|\n",
      "|977324|957460| 1909|\n",
      "|991179|870292| 1714|\n",
      "|981306|940362| 1521|\n",
      "|969751|815388| 1504|\n",
      "|990305|939606| 1464|\n",
      "|970511|825174| 1441|\n",
      "|969751|940951| 1365|\n",
      "|972032|854531| 1276|\n",
      "|972032|879259| 1253|\n",
      "|969330|936124|  910|\n",
      "|977370|927170|  910|\n",
      "|983514|930358|  870|\n",
      "+------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tracksPerArtist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, sum\n",
    "\n",
    "window = Window.partitionBy(\"artist\").orderBy(col(\"count\").desc())\n",
    "       \n",
    "topsTracksPerArtist = tracksPerArtist.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= 100) \\\n",
    "        .drop(col(\"row_number\")) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumsTracksPerArtist = topsTracksPerArtist.groupBy(col(\"artist\")) \\\n",
    ".agg(sum(col(\"count\")).alias(\"sum_weights\")) \\\n",
    ".orderBy(\"artist\") \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_topsTracksPerArtist = topsTracksPerArtist.join(sumsTracksPerArtist, \"artist\", \"inner\") \\\n",
    "    .withColumn(\"norm_weight\", col(\"count\") / col(\"sum_weights\")) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = normalized_topsTracksPerArtist.orderBy(col(\"norm_weight\").desc(), col(\"artist\"), col(\"track\")) \\\n",
    "    .limit(40) \\\n",
    "    .select(col(\"artist\"),col(\"track\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967993 869415\n",
      "967998 947428\n",
      "968004 927380\n",
      "968017 859321\n",
      "968022 852786\n",
      "968034 807671\n",
      "968038 964150\n",
      "968042 835935\n",
      "968043 913568\n",
      "968046 935077\n",
      "968047 806127\n",
      "968065 907906\n",
      "968073 964586\n",
      "968086 813446\n",
      "968092 837129\n",
      "968118 914441\n",
      "968125 821410\n",
      "968140 953008\n",
      "968148 877445\n",
      "968161 809793\n",
      "968163 803065\n",
      "968168 876119\n",
      "968189 858639\n",
      "968221 896937\n",
      "968224 892880\n",
      "968232 825536\n",
      "968237 932845\n",
      "968238 939177\n",
      "968241 879045\n",
      "968242 911250\n",
      "968248 953554\n",
      "968255 808494\n",
      "968259 880230\n",
      "968265 950148\n",
      "968266 824437\n",
      "968269 913243\n",
      "968272 816049\n",
      "968278 946743\n",
      "968285 847460\n",
      "968286 940006\n"
     ]
    }
   ],
   "source": [
    "for u, t in results.collect():\n",
    "    print(\"{} {}\".format(u, t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
