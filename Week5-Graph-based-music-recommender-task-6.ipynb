{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.enableHiveSupport().master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- trackId: integer (nullable = true)\n",
      " |-- artistId: integer (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "playHistoryPath = '/data/sample264'\n",
    "playHistoryGraph = sparkSession.read.parquet(playHistoryPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Artist: string (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metaDataPath = '/data/meta'\n",
    "metaDataGraph = sparkSession.read.parquet(metaDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "playHistoryGraph.createTempView(\"history1\")\n",
    "playHistoryGraph.createTempView(\"history2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 776748\n",
    "\n",
    "alpha = 0.15\n",
    "beta_user_artist = 0.5\n",
    "beta_user_track = 0.5\n",
    "beta_track_track = 1\n",
    "beta_artist_track = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "consecutiveTracksForUser = sparkSession.sql( \\\n",
    "    \"select h1.trackId as track1, h2.trackId as track2, h1.userId as user \" \\\n",
    "    \"from history1 h1, history2 h2 \" \\\n",
    "    \"where h1.userId = h2.userId \" \\\n",
    "    \"and h1.trackId != h2.trackId \" \\\n",
    "    \"and abs(h2.timestamp - h1.timestamp) <= 420 \" \\\n",
    ").groupBy(col(\"track1\"), col(\"track2\")) \\\n",
    ".count().alias(\"count\") \\\n",
    ".orderBy(col(\"track1\"), col(\"track2\")) \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, sum\n",
    "\n",
    "window = Window.partitionBy(\"track1\").orderBy(col(\"count\").desc())\n",
    "       \n",
    "topsDF = consecutiveTracksForUser.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= 40) \\\n",
    "        .drop(col(\"row_number\")) \\\n",
    "        .orderBy(col(\"track1\"), col(\"track2\")) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sumsDF = topsDF.groupBy(col(\"track1\")) \\\n",
    "    .agg(sum(col(\"count\")).alias(\"sum_weights\")) \\\n",
    "    .orderBy(\"track1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_count = topsDF.join(sumsDF, \"track1\", \"inner\") \\\n",
    "    .withColumn(\"weight\", col(\"count\") / col(\"sum_weights\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_track = normalized_count.withColumn(\"next_value\", col(\"weight\") * beta_track_track) \\\n",
    "    .select(\n",
    "        col(\"track1\").alias(\"source\"), \\\n",
    "        col(\"track2\").alias(\"target\"), \\\n",
    "        col(\"next_value\") \\\n",
    "    ) \\\n",
    "    .cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "tracksPerUser = sparkSession.sql( \\\n",
    "    \"select userId as user, trackId as track \" \\\n",
    "    \"from history1 \"\n",
    ").groupBy(col(\"user\"), col(\"track\")) \\\n",
    ".count().alias(\"count\") \\\n",
    ".orderBy(col(\"count\").desc(), col(\"user\"), col(\"track\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, sum\n",
    "\n",
    "window = Window.partitionBy(\"user\").orderBy(col(\"count\").desc())\n",
    "       \n",
    "topsTracksPerUser = tracksPerUser.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= 1000) \\\n",
    "        .drop(col(\"row_number\")) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumsTopsTracksPerUser = topsTracksPerUser.groupBy(col(\"user\")) \\\n",
    "    .agg(sum(col(\"count\")).alias(\"sum_weights\")) \\\n",
    "    .orderBy(\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_topsTracksPerUser = topsTracksPerUser.join(sumsTopsTracksPerUser, \"user\", \"inner\") \\\n",
    "    .withColumn(\"norm_weight\", col(\"count\") / col(\"sum_weights\")) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_track = normalized_topsTracksPerUser \\\n",
    "    .withColumn(\"next_value\", col(\"norm_weight\") * beta_user_track) \\\n",
    "    .select(col(\"user\").alias(\"source\"), \\\n",
    "            col(\"track\").alias(\"target\"), \\\n",
    "            col(\"next_value\")) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "artistsPerUser = sparkSession.sql( \\\n",
    "    \"select userId as user, artistId as artist \" \\\n",
    "    \"from history1 \"\n",
    ").groupBy(col(\"user\"), col(\"artist\")) \\\n",
    ".count().alias(\"count\") \\\n",
    ".orderBy(col(\"count\").desc(), col(\"user\"), col(\"artist\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, sum\n",
    "\n",
    "window = Window.partitionBy(\"user\").orderBy(col(\"count\").desc())\n",
    "       \n",
    "topsArtistsPerUser = artistsPerUser.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= 100) \\\n",
    "        .drop(col(\"row_number\")) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumsTopsArtistsPerUser = topsArtistsPerUser.groupBy(col(\"user\")) \\\n",
    "    .agg(sum(col(\"count\")).alias(\"sum_weights\")) \\\n",
    "    .orderBy(\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_topsArtistsPerUser = topsArtistsPerUser.join(sumsTopsArtistsPerUser, \"user\", \"inner\") \\\n",
    "    .withColumn(\"norm_weight\", col(\"count\") / col(\"sum_weights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_artist = normalized_topsArtistsPerUser \\\n",
    "    .withColumn(\"next_value\", col(\"norm_weight\")*beta_user_artist) \\\n",
    "    .select(col(\"user\").alias(\"source\"), \\\n",
    "            col(\"artist\").alias(\"target\"), \\\n",
    "            col(\"next_value\")) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "tracksPerArtist = sparkSession.sql( \\\n",
    "        \"select artistId as artist, trackId as track \" \\\n",
    "        \"from history1 \"\n",
    "    ).groupBy(col(\"artist\"), col(\"track\")) \\\n",
    "    .count().alias(\"count\") \\\n",
    "    .orderBy(col(\"count\").desc(), col(\"artist\"), col(\"track\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, sum\n",
    "\n",
    "window = Window.partitionBy(\"artist\").orderBy(col(\"count\").desc())\n",
    "       \n",
    "topsTracksPerArtist = tracksPerArtist.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= 100) \\\n",
    "        .drop(col(\"row_number\")) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumsTracksPerArtist = topsTracksPerArtist.groupBy(col(\"artist\")) \\\n",
    "    .agg(sum(col(\"count\")).alias(\"sum_weights\")) \\\n",
    "    .orderBy(\"artist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_topsTracksPerArtist = topsTracksPerArtist.join(sumsTracksPerArtist, \"artist\", \"inner\") \\\n",
    "    .withColumn(\"norm_weight\", col(\"count\") / col(\"sum_weights\")) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_track = normalized_topsTracksPerArtist \\\n",
    "    .withColumn(\"next_value\", col(\"norm_weight\")*beta_artist_track) \\\n",
    "    .select(col(\"artist\").alias(\"source\"), \\\n",
    "            col(\"track\").alias(\"target\"), \\\n",
    "            col(\"next_value\")) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = track_track \\\n",
    "    .union(user_track) \\\n",
    "    .union(user_artist) \\\n",
    "    .union(artist_track) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "user_data = playHistoryGraph.filter(col(\"userId\") == user)\n",
    "\n",
    "users = playHistoryGraph \\\n",
    "    .select(col(\"userId\").alias(\"id\")) \\\n",
    "    .distinct() \\\n",
    "    .withColumn(\"p\", when(col(\"id\") == user, 1.0).otherwise(0.0))\n",
    "\n",
    "tracks = playHistoryGraph \\\n",
    "    .select(col(\"trackId\").alias(\"id\")) \\\n",
    "    .distinct() \\\n",
    "    .join(user_data.select(col(\"trackId\").alias(\"id\"), lit(1).alias(\"temp\")).distinct(), on=\"id\", how=\"left\") \\\n",
    "    .withColumn(\"p\", when(col(\"temp\").isNotNull(), 1.0).otherwise(0.0)) \\\n",
    "    .select(col(\"id\"), col(\"p\"))\n",
    "\n",
    "artists = playHistoryGraph \\\n",
    "    .select(col(\"artistId\").alias(\"id\")) \\\n",
    "    .distinct() \\\n",
    "    .join(user_data.select(col(\"artistId\").alias(\"id\"), lit(1).alias(\"temp\")).distinct(), on=\"id\", how=\"left\") \\\n",
    "    .withColumn(\"p\", when(col(\"temp\").isNotNull(), 1.0).otherwise(0.0)) \\\n",
    "    .select(col(\"id\"), col(\"p\"))\n",
    "\n",
    "x = users \\\n",
    "    .union(artists) \\\n",
    "    .union(tracks) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = x.withColumn(\"u_prob\", when(col(\"id\") == user, 1.0).otherwise(0.0)) \\\n",
    "    .select(\"id\", \"u_prob\") \\\n",
    "    .cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "for _ in range(5):\n",
    "    sigma = x.join(edges, on=expr(\"id = source\"), how=\"left\") \\\n",
    "        .na.fill(0.0, [\"next_value\"]) \\\n",
    "        .withColumn(\"acc\", col(\"p\") * col(\"next_value\")) \\\n",
    "        .groupBy(\"target\") \\\n",
    "        .agg(sum(\"acc\").alias(\"sigma\"))\n",
    "        \n",
    "    x = u.join(sigma, on=expr(\"id = target\"), how=\"left\") \\\n",
    "         .na.fill(0.0, [\"sigma\"]) \\\n",
    "         .withColumn(\"next_value\", alpha*col(\"u_prob\") + (1-alpha) * col(\"sigma\")) \\\n",
    "         .select(col(\"id\"), col(\"next_value\").alias(\"p\")) \\\n",
    "         .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "results = x.where(\"id != \" + str(user)) \\\n",
    "    .join(metaDataGraph, on=\"id\") \\\n",
    "    .orderBy(col(\"p\").desc()) \\\n",
    "    .select(col(\"Name\"), col(\"Artist\"), round(col(\"p\"), 5).alias(\"p\")) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kill The DJ Artist: Green Day 1.42809\n",
      "Come Out and Play Artist: The Offspring 1.37473\n",
      "I Hate Everything About You Artist: Three Days Grace 1.37362\n",
      "Prayer Of The Refugee Artist: Rise Against 1.35278\n",
      "Eagle Artist: Gotthard 1.21412\n",
      "21 Guns Artist: Green Day 1.17302\n",
      "Beautiful disaster Artist: 311 0.92155\n",
      "Wait And Bleed Artist: Slipknot 0.92155\n",
      "Here To Stay Artist: Korn 0.91653\n",
      "Hard Rock Hallelujah Artist: Lordi 0.91653\n",
      "Nothing Going On Artist: Clawfinger 0.80983\n",
      "In The End Artist: Linkin Park 0.80292\n",
      "Numb Artist: Linkin Park 0.80292\n",
      "Sky is Over Artist: Serj Tankian 0.68799\n",
      "Kryptonite Artist: 3 Doors Down 0.68799\n",
      "Take It Out On Me Artist: Thousand Foot Krutch 0.47024\n",
      "Girls and Boys Artist: Blur 0.40245\n",
      "Cocaine Artist: Nomy 0.20893\n",
      "Getting Away With Murder Artist: Papa Roach 0.20648\n",
      "Artist: Green Day Artist: Green Day 0.01181\n",
      "Artist: Clawfinger Artist: Clawfinger 0.00472\n",
      "Artist: The Offspring Artist: The Offspring 0.00472\n",
      "Artist: Linkin Park Artist: Linkin Park 0.00472\n",
      "The Vengeful One Artist: Disturbed 0.00437\n",
      "She Keeps Me Up Artist: Nickelback 0.00437\n",
      "Sunday Artist: Iggy Pop 0.00437\n",
      "Artist: Iggy Pop Artist: Iggy Pop 0.00236\n",
      "Artist: Gotthard Artist: Gotthard 0.00236\n",
      "Artist: Slipknot Artist: Slipknot 0.00236\n",
      "Artist: Thousand Foot Krutch Artist: Thousand Foot Krutch 0.00236\n",
      "Artist: Papa Roach Artist: Papa Roach 0.00236\n",
      "Artist: Blur Artist: Blur 0.00236\n",
      "Artist: Rise Against Artist: Rise Against 0.00236\n",
      "Artist: Nickelback Artist: Nickelback 0.00236\n",
      "Artist: Serj Tankian Artist: Serj Tankian 0.00236\n",
      "Artist: Three Days Grace Artist: Three Days Grace 0.00236\n",
      "Artist: Disturbed Artist: Disturbed 0.00236\n",
      "Artist: 311 Artist: 311 0.00236\n",
      "Artist: Korn Artist: Korn 0.00236\n",
      "Artist: Nomy Artist: Nomy 0.00236\n"
     ]
    }
   ],
   "source": [
    "for name, artist, p in results.limit(40).collect():\n",
    "    print(\"{} {} {}\".format(name, artist, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
