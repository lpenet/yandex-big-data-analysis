{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.enableHiveSupport().master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- trackId: integer (nullable = true)\n",
      " |-- artistId: integer (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "playHistoryPath = '/data/sample264'\n",
    "playHistoryGraph = sparkSession.read.parquet(playHistoryPath)\n",
    "playHistoryGraph.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Artist: string (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metaDataPath = '/data/meta'\n",
    "metaDataGraph = sparkSession.read.parquet(metaDataPath)\n",
    "metaDataGraph.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "playHistoryGraph.createTempView(\"history1\")\n",
    "playHistoryGraph.createTempView(\"history2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "consecutiveTracksForUser = sparkSession.sql( \\\n",
    "    \"select h1.trackId as track1, h2.trackId as track2, h1.userId as user \" \\\n",
    "    \"from history1 h1, history2 h2 \" \\\n",
    "    \"where h1.userId = h2.userId \" \\\n",
    "    \"and h1.trackId != h2.trackId \" \\\n",
    "    \"and abs(h2.timestamp - h1.timestamp) <= 420 \" \\\n",
    ").groupBy(col(\"track1\"), col(\"track2\")) \\\n",
    ".count().alias(\"count\") \\\n",
    ".orderBy(col(\"track1\"), col(\"track2\")) \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consecutiveTracksForUser.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, sum\n",
    "\n",
    "window = Window.partitionBy(\"track1\").orderBy(col(\"count\").desc())\n",
    "       \n",
    "topsDF = consecutiveTracksForUser.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= 40) \\\n",
    "        .drop(col(\"row_number\")) \\\n",
    "        .orderBy(col(\"track1\"), col(\"track2\")) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sumsDF = topsDF.groupBy(col(\"track1\")) \\\n",
    ".agg(sum(col(\"count\")).alias(\"sum_weights\")) \\\n",
    ".orderBy(\"track1\") \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sumsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_count = topsDF.join(sumsDF, \"track1\", \"inner\") \\\n",
    "    .withColumn(\"weight\", col(\"count\") / col(\"sum_weights\")) \\\n",
    "    .cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = normalized_count.orderBy(col(\"weight\").desc(), col(\"track1\"), col(\"track2\")).limit(40)\n",
    "#results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.select(col(\"track1\"), col(\"track2\"))\n",
    "for t1, t2 in results.collect():\n",
    "    print(\"{}\\t{}\".format(t1,t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "tracksPerUser = sparkSession.sql( \\\n",
    "    \"select userId as user, trackId as track \" \\\n",
    "    \"from history1 \"\n",
    ").groupBy(col(\"user\"), col(\"track\")) \\\n",
    ".count().alias(\"count\") \\\n",
    ".orderBy(col(\"count\").desc(), col(\"user\"), col(\"track\")) \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracksPerUser.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, sum\n",
    "\n",
    "window = Window.partitionBy(\"user\").orderBy(col(\"count\").desc())\n",
    "       \n",
    "topsTracksPerUser = tracksPerUser.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= 1000) \\\n",
    "        .drop(col(\"row_number\")) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumsTopsTracksPerUser = topsTracksPerUser.groupBy(col(\"user\")) \\\n",
    ".agg(sum(col(\"count\")).alias(\"sum_weights\")) \\\n",
    ".orderBy(\"user\") \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_topsTracksPerUser = topsTracksPerUser.join(sumsTopsTracksPerUser, \"user\", \"inner\") \\\n",
    "    .withColumn(\"norm_weight\", col(\"count\") / col(\"sum_weights\")) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = normalized_topsTracksPerUser.orderBy(col(\"norm_weight\").desc(), col(\"user\"), col(\"track\")) \\\n",
    "    .limit(40) \\\n",
    "    .select(col(\"user\"),col(\"track\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, t in results.collect():\n",
    "    print(\"{} {}\".format(u, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "artistsPerUser = sparkSession.sql( \\\n",
    "    \"select userId as user, artistId as artist \" \\\n",
    "    \"from history1 \"\n",
    ").groupBy(col(\"user\"), col(\"artist\")) \\\n",
    ".count().alias(\"count\") \\\n",
    ".orderBy(col(\"count\").desc(), col(\"user\"), col(\"artist\")) \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+\n",
      "|  user| artist|count|\n",
      "+------+-------+-----+\n",
      "|668849| 994686|  277|\n",
      "|436158|1003021|  142|\n",
      "|442306|1001300|  107|\n",
      "|560428| 975695|   94|\n",
      "|767478| 991179|   94|\n",
      "|278647| 981306|   87|\n",
      "|770607| 978956|   76|\n",
      "| 20167| 978800|   75|\n",
      "| 26976|1003021|   75|\n",
      "|637446| 978288|   71|\n",
      "|397054| 974777|   70|\n",
      "|407962| 968823|   70|\n",
      "|201788| 970240|   69|\n",
      "|343313| 968823|   69|\n",
      "|510688| 976986|   69|\n",
      "|408783| 978956|   67|\n",
      "|714890| 985755|   67|\n",
      "|525436| 976384|   66|\n",
      "|607295| 993789|   63|\n",
      "|300275| 989189|   62|\n",
      "+------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artistsPerUser.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, sum\n",
    "\n",
    "window = Window.partitionBy(\"user\").orderBy(col(\"count\").desc())\n",
    "       \n",
    "topsArtistsPerUser = artistsPerUser.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= 100) \\\n",
    "        .drop(col(\"row_number\")) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumsTopsArtistsPerUser = topsArtistsPerUser.groupBy(col(\"user\")) \\\n",
    ".agg(sum(col(\"count\")).alias(\"sum_weights\")) \\\n",
    ".orderBy(\"user\") \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_topsArtistsPerUser = topsArtistsPerUser.join(sumsTopsArtistsPerUser, \"user\", \"inner\") \\\n",
    "    .withColumn(\"norm_weight\", col(\"count\") / col(\"sum_weights\")) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = normalized_topsArtistsPerUser.orderBy(col(\"norm_weight\").desc(), col(\"user\"), col(\"artist\")) \\\n",
    "    .limit(40) \\\n",
    "    .select(col(\"user\"),col(\"artist\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 993426\n",
      "116 974937\n",
      "128 1003021\n",
      "131 983068\n",
      "195 997265\n",
      "215 991696\n",
      "235 990642\n",
      "288 1000564\n",
      "300 1003362\n",
      "321 986172\n",
      "328 967986\n",
      "333 1000416\n",
      "346 982037\n",
      "356 974846\n",
      "374 1003167\n",
      "428 993161\n",
      "431 969340\n",
      "445 970387\n",
      "488 970525\n",
      "542 969751\n",
      "612 987351\n",
      "617 970240\n",
      "649 973851\n",
      "658 973232\n",
      "662 975279\n",
      "698 995788\n",
      "708 968848\n",
      "746 972032\n",
      "747 972032\n",
      "776 997265\n",
      "784 969853\n",
      "806 995126\n",
      "811 996436\n",
      "837 989262\n",
      "901 988199\n",
      "923 977066\n",
      "934 990860\n",
      "957 991171\n",
      "989 975339\n",
      "999 968823\n"
     ]
    }
   ],
   "source": [
    "for u, t in results.collect():\n",
    "    print(\"{} {}\".format(u, t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
